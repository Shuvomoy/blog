<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
     <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
    <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/tufte.css">
<link rel="stylesheet" href="/css/latex.css">
<link rel="stylesheet" href="/css/adjust.css"> <!-- sheet to overwrite some clashing styles -->
<link rel="icon" href="/assets/favicon.png">

     <title>Computing proximal operator of a constrained function in Julia</title>  
</head>

<body>
<div id="layout">
  <div id="menu">
    <ul>
      <li><a href="/">Home</a></li>
      <li><a href="/tags/">Tags</a></li>
    </ul>
  </div>
  <div id="main">



<!-- Content appended here -->
<div class="franklin-content"><h1 id="computing_proximal_operator_of_a_constrained_function_in_julia"><a href="#computing_proximal_operator_of_a_constrained_function_in_julia" class="header-anchor">Computing proximal operator of a constrained function in Julia</a></h1>
<p><strong>Shuvomoy Das Gupta</strong></p>
<p><em>September 24, 2020</em></p>
<hr />
<p><strong>Table of contents</strong> <div class="franklin-toc"><ol><li><a href="#example_1_low-rank_factor_analysis">Example 1: Low-rank factor analysis</a><ol><li><a href="#computing_the_proximal_operator_of_f">Computing the proximal operator of \(f\)</a></li><li><a href="#load_the_packages">Load the packages</a></li><li><a href="#solver_function">Solver function</a><ol><li><a href="#first_implementation_using_convexjl">First implementation using <code>Convex.jl</code></a></li><li><a href="#second_implementation_using_jump">Second implementation using <code>JuMP</code></a></li></ol></li><li><a href="#create_data_to_test">Create data to test</a></li><li><a href="#test_the_function">Test the function</a></li></ol></li><li><a href="#example_2_lifted_sdp_relaxation_of_a_standard_form_0-1_integer_optimization_problem">Example 2. Lifted SDP relaxation of a standard form 0-1 integer optimization problem</a><ol><li><a href="#computing_the_proximal_operator_of_f__2">Computing the proximal operator of \(f\)</a></li><li><a href="#load_the_packages__2">Load the packages</a><ol><li><ol><li><a href="#converting_jmd_file_to_jl_file">Converting <code>.jmd</code> file to <code>.jl</code> file</a></li></ol></li></ol></li></ol></li></ol></div></p>
<hr />
<p>&#40;Last update: January 13, 2021&#41; In this blog, we will show how to compute proximal operator of a constrained function. The entire code is written in markdown using the package <a href="https://github.com/JunoLab/Weave.jl">Weave.jl</a>. The corresponding <code>.jmd</code> notebook &#40;can be opened both as a markdown file in any text editor or code in <code>Juno</code>&#41;, can be downloaded <a href="https://raw.githubusercontent.com/Shuvomoy/blog/gh-pages/codes/2020-09-08-proximal_operator_over_matrix.jmd">here</a>.</p>
<p>We will consider two examples, the first one arises in low-rank factor analysis problem, and the second one comes from lifted problem of a standard form binary integer optimization problem.</p>
<h3 id="example_1_low-rank_factor_analysis"><a href="#example_1_low-rank_factor_analysis" class="header-anchor">Example 1: Low-rank factor analysis</a></h3>
<p>As an example we consider the function:</p>
\[
f(X,D) =\left\Vert \Sigma-X-D\right\Vert _{F}^{2}+I_{\mathcal{P}}(X,D),
\]
<p>where \(I_{\mathcal{P}}\) denotes the indicator function of the convex set</p>
\[
\mathcal{P}=\{(X,D)\in\mathbf{S}^{n}\times\mathbf{S}^{n}\mid X\succeq0,D=\mathbf{diag}(d),d\geq0,d\in \mathbf{R}^{n}, \Sigma-D \succeq 0\}.
\]
<h4 id="computing_the_proximal_operator_of_f"><a href="#computing_the_proximal_operator_of_f" class="header-anchor">Computing the proximal operator of \(f\)</a></h4>
<p>Proximal operator \(\mathbf{prox}_{\gamma f}\) for this function \(f\) at \((X,D)\) is <em>the</em> optimal solution to the following convex optimization problem:</p>
\[
\begin{equation}
\begin{array}{ll}
\textrm{minimize} & \left\Vert \Sigma-\widetilde{X}-\widetilde{D}\right\Vert _{F}^{2}+\frac{1}{2\gamma}\|\widetilde{X}-X\|_{F}^{2}+\frac{1}{2\gamma}\|\widetilde{D}-D\|_{F}^{2}\\
\textrm{subject to} & \widetilde{X}\succeq0\\
 & \widetilde{D}=\mathbf{diag}(\widetilde{d})\\
 & \widetilde{d}\geq0, \\
 & \Sigma - \widetilde{D} \succeq 0
\end{array}
\end{equation}
\]
<p>where \(\widetilde{X}\in\mathbf{S}_{+}^{n},\) and \(\widetilde{d}\in \mathbf{R}_{+}^{n}\)&#40;<em>i.e.</em>, \(\widetilde{D}=\mathbf{diag}(\widetilde{d}\)&#41;&#41; are the optimization variables.</p>
<p>Now we solve this optimization problem using <code>Julia</code>. We will use the package <code>Convex</code> and <code>SCS</code>, both open source <code>Julia</code> packages.</p>
<h4 id="load_the_packages"><a href="#load_the_packages" class="header-anchor">Load the packages</a></h4>
<p>First, we load the packages. If the packages are not installed we can install them by running the following commands in <code>Julia</code> REPL.</p>
<pre><code class="language-julia">using Pkg
Pkg.add&#40;&quot;Convex&quot;&#41;
Pkg.add&#40;&quot;COSMO&quot;&#41;</code></pre>
<pre><code class="language-julia">## Load the packages
using Convex
using LinearAlgebra
using COSMO
using JuMP
using SCS</code></pre>
<h4 id="solver_function"><a href="#solver_function" class="header-anchor">Solver function</a></h4>
<p>Let us write the solver function that is going to solve the optimization problem that we described above. The first implementation is using <code>Convex.jl</code> and the second one is via <code>JuMP</code>.</p>
<h5 id="first_implementation_using_convexjl"><a href="#first_implementation_using_convexjl" class="header-anchor">First implementation using <code>Convex.jl</code></a></h5>
<pre><code class="language-julia"># put everything in a function: first implementatino is using Convex.jl
function prox_PRS_fam_cvxjl&#40;Σ, M, γ, X, d&#41; #&#40;Σ::A, M::R, γ::R, X::A, d::V&#41; where &#123;R &lt;: Real, A &lt;: AbstractMatrix&#123;R&#125;, V &lt;:  AbstractVector&#123;R&#125;&#125; # For now M is not used, may use it in a future version

  # This functions takes the input data Σ, γ, X, d and evaluates
  # the proximal operator of the function f at the point &#40;X,D&#41;

  # Data extraction
  # ---------------
  n &#61; length&#40;d&#41; # dimension of the problem
  # println&#40;&quot;*****************************&quot;&#41;
  # println&#40;size&#40;d&#41;&#41;
  # println&#40;&quot;the value of d is &#61; &quot;, d&#41;
  # println&#40;&quot;the type of d is&quot;, typeof&#40;d&#41;&#41;
  D &#61; LinearAlgebra.diagm&#40;d&#41; # creates the diagonal matrix D that embed

  # Create the variables
  #  --------------------
  X_tl &#61; Convex.Semidefinite&#40;n&#41; # Here Semidefinite&#40;n&#41; encodes that
  # X_tl ≡ ̃X is a positive semidefinite matrix
  d_tl &#61; Convex.Variable&#40;n&#41; # d_tl ≡ ̃d
  D_tl &#61; diagm&#40;d_tl&#41; # Create the diagonal matrix ̃D from ̃d

  # Create terms of the objective function, which we write down
  #  in three parts
  #  ----------------------------------------------------------
  t1 &#61; square&#40;norm2&#40;Σ - X_tl - D_tl&#41;&#41; # norm2 computes Frobenius norm in Convex.jl
  t2 &#61; square&#40;norm2&#40;X-X_tl&#41;&#41;
  t3 &#61; square&#40;norm2&#40;D-D_tl&#41;&#41;

  # Create objective
  # ----------------
  objective &#61; t1 &#43; &#40;1/&#40;2*γ&#41;&#41;*&#40;t2 &#43; t3&#41; # the objective to be minimized

  # create the problem instance
  # ---------------------------
  convex_problem &#61; Convex.minimize&#40;objective, &#91;d_tl &gt;&#61; 0, Σ - D_tl  in :SDP&#93;&#41;

  # set the solver
  # --------------
  convex_solver &#61; &#40;&#41; -&gt; SCS.Optimizer&#40;verbose&#61;false&#41;

  # solve the problem
  # -----------------
  Convex.solve&#33;&#40;convex_problem, convex_solver&#41;

  # get the optimal solution
  # ------------------------
  X_sol &#61; X_tl.value
  d_sol &#61; d_tl.value
  # println&#40;&quot;d_sol &#61; &quot;, d_sol&#41;

  # return the output
  return X_sol, vec&#40;d_sol&#41;

end</code></pre>
<h5 id="second_implementation_using_jump"><a href="#second_implementation_using_jump" class="header-anchor">Second implementation using <code>JuMP</code></a></h5>
<pre><code class="language-julia">## put everything in a function &#40;implementation using JuMP&#41;
function prox_PRS_fam_JuMP&#40;Σ, M, γ, X, d; X_tl_sv &#61; nothing, d_tl_sv &#61; nothing, warm_start &#61; false&#41;

	# This functions takes the input data Σ, γ, X, d and evaluates the proximal operator of the function f at the point &#40;X,d&#41;

	n &#61; length&#40;d&#41;

	prox_model &#61; JuMP.Model&#40;optimizer_with_attributes&#40;SCS.Optimizer, &quot;verbose&quot; &#61;&gt; false&#41;&#41;

	# prox_model &#61; JuMP.Model&#40;optimizer_with_attributes&#40;Mosek.Optimizer&#41;&#41;


	@variables&#40; prox_model,
	begin
		d_tl&#91;1:n&#93; &gt;&#61; 0
		X_tl&#91;1:n, 1:n&#93;, PSD
	end
	&#41;

	if warm_start &#61;&#61; true
		println&#40;&quot;warm start enabled&quot;&#41;
		# Set warm-start
		set_start_value.&#40;X_tl, X_tl_sv&#41; # Warm start
		set_start_value.&#40;d_tl, d_tl_sv&#41; # Warm start
	    println&#40;&quot;norm difference is &#61; &quot;, norm&#40;start_value.&#40;X_tl&#41; - X_tl_sv&#41;&#41;
	end



    t_1 &#61; vec&#40;Σ - X_tl - diagm&#40;d_tl&#41;&#41;
	t_2 &#61; vec&#40;X_tl-X&#41;
	t_3 &#61; vec&#40;diagm&#40;d_tl&#41;-diagm&#40;d&#41;&#41;
	obj &#61; t_1&#39;*t_1 &#43; &#40;&#40;1/&#40;2*γ&#41;&#41;*&#40;t_2&#39;*t_2 &#43; t_3&#39;*t_3&#41;&#41;

	@objective&#40;prox_model, Min, obj&#41;

	@constraints&#40;prox_model, begin
		Symmetric&#40;Σ - diagm&#40;d_tl&#41;&#41; in PSDCone&#40;&#41;
	end&#41;

	set_silent&#40;prox_model&#41;

	JuMP.optimize&#33;&#40;prox_model&#41;

	# obj_val &#61; JuMP.objective_value&#40;prox_model&#41;
	X_sol &#61; JuMP.value.&#40;X_tl&#41;
	d_sol &#61; JuMP.value.&#40;d_tl&#41;

	return X_sol, d_sol

end</code></pre>
<h4 id="create_data_to_test"><a href="#create_data_to_test" class="header-anchor">Create data to test</a></h4>
<p>We create the data now to test the function we just wrote.</p>
<pre><code class="language-julia">## All the parameters
n &#61; 10
Σ1 &#61; randn&#40;n,n&#41;
Σ &#61; Σ1&#39;*Σ1
X &#61; randn&#40;n,n&#41;
d &#61; randn&#40;n&#41;
M &#61; 1
γ &#61; 1</code></pre>
<h4 id="test_the_function"><a href="#test_the_function" class="header-anchor">Test the function</a></h4>
<p>We test the function now to see if the function <code>prox_over_matrix</code> works as expected&#33;</p>
<pre><code class="language-julia">## Time to run the code
@time X1, d1 &#61; prox_PRS_fam_cvxjl&#40;Σ, M, γ, X, d&#41;

@time X2, d2 &#61; prox_PRS_fam_JuMP&#40;Σ, M, γ, X, d&#41;

## Test for warmstarting

X_tl_sv &#61; X2
d_tl_sv &#61; d2

@time X2, d2 &#61; prox_PRS_fam_JuMP&#40;Σ, M, γ, X, d; X_tl_sv &#61; X2, d_tl_sv &#61; d2, warm_start &#61; true&#41;</code></pre>
<code>Output:</code>
<pre><code class="language-julia">2.263578 seconds &#40;11.69 k allocations: 1.033 MiB&#41;
  1.613454 seconds &#40;18.60 k allocations: 2.731 MiB&#41;
  warm start enabled
  norm difference is &#61; 0.0
  1.643022 seconds &#40;19.30 k allocations: 2.764 MiB&#41;</code></pre>
<h3 id="example_2_lifted_sdp_relaxation_of_a_standard_form_0-1_integer_optimization_problem"><a href="#example_2_lifted_sdp_relaxation_of_a_standard_form_0-1_integer_optimization_problem" class="header-anchor">Example 2. Lifted SDP relaxation of a standard form 0-1 integer optimization problem</a></h3>
<p>The standard form binary integer optimization problem is a universal combinatorial problem with the following form:</p>
\[
\begin{equation}
\begin{array}{ll}
\textrm{minimize} & c^{\top}x\\
\textrm{subject to} & Ax=b\\
 & x\geq0\\
 & x\in\{0,1\}^{n},
\end{array}
\end{equation}
\]
<p>where \(x\) is the decision variable, and \(A\in\mathbf{R}^{m\times n},b\in\mathbf{R}^{m},c\in\mathbf{R}^{n}\) are problem data. The matrix \(A\) is taken to be fat \((m<n)\) and full row rank. This problem has the following higher-dimensional equivalent formulation based on the lifting procedure originally proposed by Lovász and Schrijver:</p>
\[
\begin{equation}
\begin{array}{ll}
\textrm{minimize} & \sum_{j=1}^{n}c_{j}X_{jj}\\
\textrm{subject to} & \sum_{j=1}^{n}A_{j}X_{ij}=bX_{ii},\qquad i=1,\ldots,n,\\
 & \sum_{j=1}^{n}A_{j}X_{jj}=b\\
 & 0\leq X_{ij}\leq X_{ii},\qquad i,j=1,\ldots,n,i\neq j,\\
 & 0\leq X_{ij}\leq1,\qquad i,j=1,\ldots,n,\\
 & X_{ii}+X_{jj}-X_{ij}\leq1,\qquad i,j=1,\ldots,n,i\neq j,\\
 & X\succeq0,\\
 & \mathbf{rank} X=1,
\end{array}
\end{equation}
\]
<p>where \(X\in\mathbf{S}^{n}\) is the decision variable and is related to the original decision variable \(x\) by the encoding \(X=xx^{\top}.\) In this formulation \(A_{j}\) represents the \(j\)-th column of the matrix \(A\).</p>
<p>Let us drop the nonconvex rank constraint, and consider the function</p>
\[
f(X) :=\sum_{j=1}^{n}c_{j}X_{jj} +I_{\mathcal{C}}(X),
\]
<p>where \(I_{\mathcal{C}}\) denotes the indicator function of the convex set</p>
\[
\begin{equation}
\begin{array}{ll}
\mathcal{C} := & \{ X\in\mathbf{S}^{n}\mid\sum_{j=1}^{n}A_{j}X_{ij}=bX_{ii},\qquad i\in1,\ldots,n,\\
 & \qquad\qquad\sum_{j=1}^{n}A_{j}X_{jj}=b,\\
 & \qquad\qquad 0\leq X_{ij}\leq X_{ii},\qquad i,j=1,\ldots,n,i\neq j,\\
 & \qquad \qquad0\leq X_{ij}\leq1,\qquad i,j=1,\ldots,n\\
 & \qquad \qquad X_{ii}+X_{jj}-X_{ij}\leq1,\qquad i,j=1,\ldots,n,i\neq j,\\
 & \qquad \qquad X\succeq0 \}.
\end{array}
\end{equation}
\]
<h4 id="computing_the_proximal_operator_of_f__2"><a href="#computing_the_proximal_operator_of_f__2" class="header-anchor">Computing the proximal operator of \(f\)</a></h4>
<p>Proximal operator \(\mathbf{prox}_{\gamma f}\) for this function \(f\) at \(X\) is <em>the</em> optimal solution to the following convex optimization problem:</p>
\[
\begin{equation}
\begin{array}{ll}
\textrm{minimize} & \sum_{j=1}^{n}c_{j}\widetilde{X}_{jj}+\frac{1}{2\gamma}\|\widetilde{X}-X\|_{F}^{2}\\
\textrm{subject to} & \sum_{j=1}^{n}A_{j}\widetilde X_{ij}=b \widetilde X_{ii},\qquad i=1,\ldots,n,\\
 & \sum_{j=1}^{n}A_{j} \widetilde X_{jj}=b\\
 & 0\leq \widetilde X_{ij}\leq \widetilde X_{ii},\qquad i,j=1,\ldots,n,i\neq j,\\
 & 0\leq \widetilde X_{ij}\leq1,\qquad i,j=1,\ldots,n,\\
 & \widetilde X_{ii}+\widetilde X_{jj}-\widetilde X_{ij}\leq1,\qquad i,j=1,\ldots,n,i\neq j,\\
 & \widetilde X\succeq0,\\

\end{array}
\end{equation}
\]
<p>where \(\widetilde{X}\in\mathbf{S}_{+}^{n}\) is the optimization variables.</p>
<h4 id="load_the_packages__2"><a href="#load_the_packages__2" class="header-anchor">Load the packages</a></h4>
<p>First, we load the packages.</p>
<pre><code class="language-julia">## Load the packages
using Convex
using LinearAlgebra
using COSMO
using JuMP
using SCS</code></pre>
<pre><code class="language-julia"># Problem Data
m &#61; 5
n &#61; 10
using Random: bitrand
c &#61; randn&#40;n&#41;
x0 &#61; bitrand&#40;n&#41;
A &#61; randn&#40;m,n&#41;
b &#61; A*x0 # this is to ensure that the problem has a solution
x_test &#61; randn&#40;n&#41;
X &#61; x_test*x_test&#39; # this is the point where we want to evaluate the proximal operator
γ &#61; 1 # proximal parameter</code></pre>
<pre><code class="language-julia"># Evaluating the proximal operator &#40;put the code in a function&#41;
prox_model &#61; JuMP.Model&#40;optimizer_with_attributes&#40;Mosek.Optimizer&#41;&#41;

@variables&#40;prox_model,
begin
	X_tl&#91;1:n, 1:n&#93;, PSD
end
&#41;

t_1 &#61; sum&#40;c&#91;j&#93;*X_tl&#91;j,j&#93; for j in 1:n&#41;
t_2 &#61; vec&#40;X_tl-X&#41;
obj &#61; t_1 &#43; &#40;&#40;1/&#40;2*γ&#41;&#41;*&#40;t_2&#39;*t_2&#41;&#41;

@objective&#40;prox_model, Min, obj&#41;

@constraints&#40;prox_model, begin
con_sdp_equality_1&#91;i &#61; 1:n&#93;, sum&#40;X_tl&#91;j,j&#93;.*A&#91;:,j&#93; for j in 1:n&#41; .&#61;&#61; b*X_tl&#91;i,i&#93;
con_sdp_equality_2, sum&#40;X_tl&#91;j,j&#93;.*A&#91;:,j&#93; for j in 1:n&#41; .&#61;&#61; b
con_bound&#91;i &#61; 1:n, j &#61; 1:n&#93;, 0&lt;&#61; X_tl&#91;i,j&#93; &lt;&#61; 1
cons_sdp_bound_1&#91;i &#61; 1:n, j&#61; 1:n, i &#33;&#61; j&#93;, X_tl&#91;i,i&#93;&#43;X_tl&#91;j,j&#93;-X_tl&#91;i,j&#93; &lt;&#61; 1
con_sdp_bound_2&#91;i &#61; 1:n, j &#61; 1:n, i &#33;&#61; j&#93;, X_tl&#91;i,j&#93; &lt;&#61; X_tl&#91;i,i&#93;
end&#41;

set_silent&#40;prox_model&#41;

JuMP.optimize&#33;&#40;prox_model&#41;

obj_val &#61; JuMP.objective_value&#40;prox_model&#41;

X_sol &#61; JuMP.value.&#40;X_tl&#41;</code></pre>
<h6 id="converting_jmd_file_to_jl_file"><a href="#converting_jmd_file_to_jl_file" class="header-anchor">Converting <code>.jmd</code> file to <code>.jl</code> file</a></h6>
<p>To convert the <code>.jmd</code> file to a <code>.jl</code> file we run the following code:</p>
<pre><code class="language-julia">using Weave
cd&#40;&quot;C:\\Users\\shuvo\\Google Drive\\GitHub\\blog\\codes&quot;&#41; # directory that contains the .jmd file
tangle&#40;&quot;2020-09-08-proximal_operator_over_matrix.jmd&quot;, informat &#61; &quot;markdown&quot;&#41; # convert the .jmd file into a .jl file that will contain the code</code></pre>
<div class="page-foot">
  <div class="copyright">
    &copy; Shuvomoy Das Gupta. Last modified: May 20, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
        </div> <!-- end of id=main -->
    </div> <!-- end of id=layout -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
