{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a simple gradient descent algorithm in ``Julia``"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a differentiable convex function $f$, our goal is to solve $\\textrm{minimize}\\;f(x)$, where $x\\in \\mathbf{R}^n$ is the decision variable. To solve the problem above, we consider gradient descent algorithm. The gradient descent implements the following iteration scheme:\n",
        "\n",
        "$\n",
        "x_{n+1}  =  x_{n}-\\gamma_{n}{\\nabla f(x_{n})},\\qquad (1)\n",
        "$\n",
        "\n",
        "where ${\\nabla f(x_{n})}$ denotes a gradient of $f$ evaluated at the iterate $x_{n}$, and $n$ is our iteration counter. As our step size rule, we pick a sequence that is square-summable but not summable, e.g., $\\gamma_{n}=1/n$, will do the job. \n",
        "\n",
        "We will go through the following steps:\n",
        "1. Load the packages\n",
        "2. Create the types\n",
        "3. Write the functions\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the packages\n",
        "Let us load the necessary packages that we are going to use."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the packages to be used\n",
        "# -----------------------------\n",
        "using ProximalOperators, LinearAlgebra"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create the types\n",
        "\n",
        "Next, we define a few Julia types, that we require to write an optimization solver in `Julia`. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1. ``GD_problem``\n",
        "\n",
        "This type contains information about the problem instance, this bascially tells us what function $f$ we are trying to optimize over, one initial point $x_0$, and what should be the beginning step size $\\gamma_0$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "struct GD_problem \n",
        "    \n",
        "    # problem structure, contains information regarding the problem\n",
        "    \n",
        "    f # the objective function\n",
        "    x0 # the intial condition\n",
        "    γ # the stepsize\n",
        "    \n",
        "end"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage of ``GD_problem``.**  For example, the user may wish to solve a simple least-squares problem using gradient descent. Then he can create a problem instance. A list of functions that we can use in this regard can be found in the documentation of ``ProximalOperators``: [https://kul-forbes.github.io/ProximalOperators.jl/latest](https://kul-forbes.github.io/ProximalOperators.jl/latest)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create a problem instance\n",
        "# ------------------------\n",
        "\n",
        "A = randn(6,5)\n",
        "\n",
        "b = randn(6)\n",
        "\n",
        "m, n = size(A)\n",
        "\n",
        "# randomized intial point:\n",
        "\n",
        "x0 = randn(n)\n",
        "\n",
        "f = LeastSquares(A, b)\n",
        "\n",
        "γ = 1.0\n",
        "\n",
        "# create GD_problem\n",
        "\n",
        "problem = GD_problem(f, x0, γ)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": [
              "GD_problem(description : Least squares penalty\n",
              "domain      : n/a\n",
              "expression  : n/a\n",
              "parameters  : n/a, [1.8897906311257238, -0.08306021415622838, 1.5446871907852175, -0.3901442030510987, -0.13333660370434405], 1.0)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. ``GD_setting`` \n",
        "\n",
        "This type contains different parameters required to implement our algorithm, such as, \n",
        "\n",
        "* the initial step size $\\gamma$, \n",
        "* maximum number of iterations $\\textrm{maxit}$, \n",
        "* what should be the tolerance $\\textrm{tol}$ (i.e., if $\\| \\nabla{f(x)} \\| \\leq \\textrm{tol}$, we take that $x$ to be an optimal solution and terminate our algorithm), \n",
        "* whether to print out information about  the iterates or not controlled by a boolean variable $\\textrm{verbose}$, and \n",
        "* how frequently to print out such information controlled by the variable $\\textrm{freq}$.\n",
        "\n",
        "The user may specify what values for these parameters above should be used. But if he does not specify anything, we should be able to have a default set of values to be used. We can achieve this by creating a simple constructor function for ``GD_setting``."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "struct GD_setting\n",
        "    \n",
        "    # user settings to solve the problem using Gradient Descent\n",
        "    \n",
        "    γ # the step size\n",
        "    maxit # maximum number of iteration\n",
        "    tol # tolerance, i.e., if ||∇f(x)|| ≤ tol, we take x to be an optimal solution\n",
        "    verbose # whether to print information about the iterates\n",
        "    freq # how often print information about the iterates\n",
        "\n",
        "    # constructor for the structure, so if user does not specify any particular values, \n",
        "    # then we create a GD_setting object with default values\n",
        "    function GD_setting(; γ = 1, maxit = 1000, tol = 1e-8, verbose = false, freq = 10)\n",
        "        new(γ, maxit, tol, verbose, freq)\n",
        "    end\n",
        "    \n",
        "end"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage of ``GD_setting``.** For the previously described least squares problem, we create the following setting instance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "setting = GD_setting(verbose = true, tol = 1e-2, maxit = 1000, freq = 100)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": [
              "GD_setting(1, 1000, 0.01, true, 100)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. ``GD_state``\n",
        "Now we define the type named ``GD_state`` that describes the state our algorithm at iteration number $n$. The state is controlled by \n",
        "\n",
        "* current iterte $x_n$,\n",
        "* the gradient of $f$ at the current iterate: ${\\nabla{f}(x_n)}$,\n",
        "* the stepsize at iteration $n$: $\\gamma_n$, and\n",
        "* iteration number: $n$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mutable struct GD_state # contains information regarding one iterattion sequence\n",
        "    \n",
        "    x::Any # iterate x_n\n",
        "    ∇f_x::Any # one gradient ∇f(x_n)\n",
        "    γ::Any # stepsize\n",
        "    n::Any # iteration counter\n",
        "    \n",
        "end"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, once the user has given the problem information by creating a problem instance ``GD_problem``, we need a method to construct the intial value of the type `GD_state`,  as we did earlier for the least-squares problem. We create the intial state from the problem instance by writing a constructor function."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "function GD_state(problem::GD_problem) \n",
        "    # a constructor for the struct GD_state, it will take the problem data and create one state containing all \n",
        "    # the iterate information, current state of the gradient etc so that we can start our gradient descent scheme\n",
        "    \n",
        "    # unpack information from iter which is GD_iterable type\n",
        "    x0 = problem.x0\n",
        "    f = problem.f\n",
        "    γ = problem.γ\n",
        "    ∇f_x, f_x = gradient(f, x0)\n",
        "    n = 1\n",
        "    \n",
        "    return GD_state(x0, ∇f_x, γ, n)\n",
        "    \n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": [
              "GD_state"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Write the functions \n",
        "\n",
        "Now that we are done defining the types, we can now focus on writing the functions that will implement our gradient descent scheme. \n",
        "\n",
        "#### 3.1. ```GD_iteration!```\n",
        "First, we need a function that will take the problem information and the state of our algorithm at iteration number $n$, and then compute the next state for iteration number $n+1$ according to (1). "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "function GD_iteration!(problem::GD_problem, state::GD_state)\n",
        "    \n",
        "    # this is the main iteration function, that takes the problem information, and the previous state, \n",
        "    # and create the new state using Gradient Descent algorithm\n",
        "    \n",
        "    # unpack the current state information\n",
        "    x_n = state.x\n",
        "    ∇f_x_n = state.∇f_x\n",
        "    γ_n = state.γ\n",
        "    n = state.n\n",
        "    \n",
        "    # compute the next state\n",
        "    x_n_plus_1 = x_n - γ_n*∇f_x_n\n",
        "    \n",
        "    # now load the computed values in the state\n",
        "    state.x = x_n_plus_1\n",
        "    state.∇f_x, f_x = gradient(problem.f, x_n_plus_1) # note that f_x is not used anywhere\n",
        "    state.γ = 1/(n+1)\n",
        "    state.n = n+1\n",
        "    \n",
        "    # done computing return the new state\n",
        "    return state\n",
        "    \n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": [
              "GD_iteration! (generic function with 1 method)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ``GD_solver``\n",
        "Now we are in a position to write the main solver function named ``GD_solver`` that will be used by the end user. Internally, this function will take the problem information and the problem setting, and then it will\n",
        "\n",
        "* create the initial state,\n",
        "* keep updating the state using ``GD_iteration!`` function until we reach the termination criterion or the maximum number of iterations,\n",
        "* print state of the algorithm if ``verbose`` is ``true`` at the specified frequency, and \n",
        "* return the final state."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## The solver function\n",
        "\n",
        "function GD_solver(problem::GD_problem, setting::GD_setting)\n",
        "    \n",
        "    # this is the function that the end user will use to solve a particular problem, internally it is using the previously defined types and functions to run Gradient Descent Scheme\n",
        "    # create the intial state\n",
        "    state = GD_state(problem::GD_problem)\n",
        "    \n",
        "    ## time to run the loop\n",
        "    while  (state.n < setting.maxit) & (norm(state.∇f_x, Inf) > setting.tol)\n",
        "        # compute a new state\n",
        "        state =  GD_iteration!(problem, state)\n",
        "        # print information if verbose = true\n",
        "        if setting.verbose == true\n",
        "            if mod(state.n, setting.freq) == 0\n",
        "                @info \"iteration = $(state.n) |  \n",
        "                obj val = $(problem.f(state.x)) | \n",
        "                gradient norm = $(norm(state.∇f_x, Inf))\"\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    \n",
        "    # print information regarding the final state\n",
        "    \n",
        "    @info \"final iteration = $(state.n) | \n",
        "    final obj val = $(problem.f(state.x)) | \n",
        "    final gradient norm = $(norm(state.∇f_x, Inf))\"\n",
        "    return state\n",
        "    \n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": [
              "GD_solver (generic function with 1 method)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage of ``GD_solver``.** For the previously created ``problem`` and ``setting``, we run our ``GD_solver`` function as follows.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The following function will run the entire loop over the struct GradientDescent"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "final_state_GD = GD_solver(problem, setting)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: final iteration = 18 | \n",
            "│     final obj val = 1.8840226555101893 | \n",
            "│     final gradient norm = 0.0017729146193610212\n",
            "└ @ Main In[9]:25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": [
              "GD_state([-0.40875893212646097, 0.5866586203422339, 0.18948627216649985, 0.27917741589861966, -0.48461136196842236], [-0.0016798892487610573, 0.0011736858771698166, -0.001489579860942003, -0.0017729146193610212, 0.0005631929687850423], 0.05555555555555555, 18)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "println(\"objective value found by our gradient descent $(f(final_state_GD.x))\")\n",
        "\n",
        "println(\"real objective value $(f(pinv(A)*b)) \")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective value found by our gradient descent 1.8840226555101893\n",
            "real objective value 1.8840206671818387 \n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we do decent in terms of finding a good solution!"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.3.0",
      "language": "julia",
      "name": "julia-1.3"
    },
    "language_info": {
      "file_extension": ".jl",
      "name": "julia",
      "mimetype": "application/julia",
      "version": "1.3.0"
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}