{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple gradient descent method in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, we discuss how to implement a simple subgradient descent scheme in Julia using Iterators module. \n",
    "\n",
    "#### Background.\n",
    "\n",
    "Given a convex function $f$, our goal is to solve the following optimization problem: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\begin{array}{ll}\n",
    "\\textrm{minimize} & f(x)\\\\\n",
    "\\textrm{subject to} & x\\in\\mathbf{R}^{n},\n",
    "\\end{array}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $x$ is the decision variable. To solve the problem above, we consider subgradient descent algorithm. The subgradient descent implements the following iteration scheme:\n",
    "\n",
    "\\begin{eqnarray} \\label{SGD}\n",
    "x_{n+1} & = & x_{n}-\\gamma_{n}{\\nabla f(x_{n})},\\qquad (1)\n",
    "\\end{eqnarray}\n",
    "\n",
    " where ${\\nabla f(x_{n})}$ denotes one subgradient of $f$ evaluated at the iterate $x_{n}$, and $n$ is our iteration counter. As our step size rule, we pick a sequence that is square-summable but not summable, e.g., $\\gamma_{n}=1/n$, will do the job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the necessary packages that we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Iterators\n",
    "using ProximalAlgorithms.IterationTools\n",
    "using ProximalOperators: Zero\n",
    "using LinearAlgebra\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create an `iterable` for our subgradient descent method, which will contain the function $f$ to optimize over and the initial condition $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GD_iterable{R <: Real, T <: AbstractArray{R}, Tf}\n",
    "    f::Tf # the objective function\n",
    "    x0::T # the intial condition\n",
    "    γ::R # the stepsize\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the state of the algorithms is controlled by one iterte $x_n$ and the subgradient ${\\nabla{f}(x_n)}$. When the subgradient ${\\nabla{f}(x_n)}$ reaches a value that is approximately zero, we terminate our algorithm and take the corresponding $x_n$ as the approximate solution to our opitmization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct GD_state{T}\n",
    "    x # iterate x_n\n",
    "    ∇f_x # one gradient ̃̃∇f(x_n)\n",
    "    γ # stepsize\n",
    "    n # iteration counter\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an initialization mechanism for `SGD_state`, which we defnie next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD_state"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD_state(iter::GD_iterable) = GD_state(copy(iter.x), ones(iter.x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an `iterate` function that will take $(x_n, {\\nabla{f(x_n)}})$, and produce the next state $x_{n+1}$ using $(1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(iter::GD_iterable, state::GD_state)\n",
    "    # unpack the current state information\n",
    "    x_n = state.x\n",
    "    ∇f_x_n = state.∇f_x\n",
    "    γ_n = state.γ\n",
    "    n = state.n\n",
    "    # compute the next state\n",
    "    x_n_plus_1 = x_n - γ_n*∇f_x_n\n",
    "    # now load the computed values in the state\n",
    "    state.x = x_n_plus_1\n",
    "    state.∇f_x = gradient(iter.f, x_n_plus_1)\n",
    "    state.γ = 1/(n+1)\n",
    "    state.n = n+1\n",
    "    return state, state\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a simple solver, that will take a convex function $f$ and give its optimal solution $x^\\star$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GradientDescent\n",
    "    γ\n",
    "    maxit # maximum number of iteration\n",
    "    tol # tolerance, i.e., if ||∇f(x)|| ≤ tol, we take x to be an optimal solution\n",
    "    verbose # whether to print information about the iterates\n",
    "    freq # how often print information about the iterates\n",
    "    \n",
    "    # constructor for the structure\n",
    "    function GradientDescent(; γ = 1, maxit = 1000, tol = 1e-8, verbose = false, freq = 10)\n",
    "        new(γ, maxit, tol, verbose, freq)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will run the entire loop over the struct GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (solver::GradientDescent)(x0, f) \n",
    "    # x0: initial condition\n",
    "    # f: the function to be optimized over\n",
    "    \n",
    "    stop(state::GD_state) = norm(state.res, Inf) <= solver.tol\n",
    "    disp((it, state)) = @printf(\"%5d | %.3e\\n\", it, norm(state.res, Inf))\n",
    "    \n",
    "    # time to run the iteration\n",
    "    iter = GD_iterable(f, x0, solver.γ)\n",
    "    iter = take(halt(iter, stop), solver.maxit)\n",
    "    iter = enumerate(iter)\n",
    "    \n",
    "    if solver.verbose == true \n",
    "        iter = tee(sample(iter, solver.freq), disp)\n",
    "    end\n",
    "    \n",
    "    num_iters, state_final = loop(iter)\n",
    "\n",
    "    return state_final.x, state_final.∇f_x, state.γ, state.n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ProximalAlgorithms, ProximalOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = Float64\n",
    "\n",
    "A = randn(4,5)\n",
    "\n",
    "b = randn(4)\n",
    "\n",
    "m, n = size(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "  0.34314605308496215\n",
       "  0.9624123745157233 \n",
       " -0.9680974471641903 \n",
       " -0.33048683386912975\n",
       "  0.934294979136089  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized intial point:\n",
    "x0 = randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description : Least squares penalty\n",
       "domain      : n/a\n",
       "expression  : n/a\n",
       "parameters  : n/a"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientDescent(1, 1000, 1.0e-8, false, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = GradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching GD_iterable(::ProximalOperators.LeastSquaresDirect{Float64,Float64,Array{Float64,2},Array{Float64,1},Cholesky{Float64,Array{Float64,2}}}, ::Array{Float64,1}, ::Int64)\nClosest candidates are:\n  GD_iterable(::Tf, ::T, !Matched::R) where {R<:Real, T<:(AbstractArray{R,N} where N), Tf} at In[47]:2",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching GD_iterable(::ProximalOperators.LeastSquaresDirect{Float64,Float64,Array{Float64,2},Array{Float64,1},Cholesky{Float64,Array{Float64,2}}}, ::Array{Float64,1}, ::Int64)\nClosest candidates are:\n  GD_iterable(::Tf, ::T, !Matched::R) where {R<:Real, T<:(AbstractArray{R,N} where N), Tf} at In[47]:2",
      "",
      "Stacktrace:",
      " [1] (::GradientDescent)(::Array{Float64,1}, ::ProximalOperators.LeastSquaresDirect{Float64,Float64,Array{Float64,2},Array{Float64,1},Cholesky{Float64,Array{Float64,2}}}) at .\\In[53]:9",
      " [2] top-level scope at In[60]:1"
     ]
    }
   ],
   "source": [
    "solver(x0, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
