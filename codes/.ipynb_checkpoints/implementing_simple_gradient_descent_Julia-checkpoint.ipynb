{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple gradient descent algorithm in ``Julia``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, we discuss how to implement a simple gradient descent scheme in ``Julia``. To do this, we will use the Julia package ``ProximalOperators``, which is an excellent package to compute proximal operators and gradient of common convex functions. I highly recommend the package for anyone interested in operator splitting algorithms. You can find more information about the package at: [https://github.com/kul-forbes/ProximalOperators.jl](https://github.com/kul-forbes/ProximalOperators.jl).\n",
    "\n",
    "Before we implement gradient descent method, we first record some necessary background.\n",
    "\n",
    "### 0. Background.\n",
    "\n",
    "Given a differentiable convex function $f$, our goal is to solve the following optimization problem: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\begin{array}{ll}\n",
    "\\textrm{minimize} & f(x)\\\\\n",
    "\\textrm{subject to} & x\\in\\mathbf{R}^{n},\n",
    "\\end{array}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $x$ is the decision variable. To solve the problem above, we consider gradient descent algorithm. The gradient descent implements the following iteration scheme:\n",
    "\n",
    "\\begin{eqnarray} \\label{SGD}\n",
    "x_{n+1} & = & x_{n}-\\gamma_{n}{\\nabla f(x_{n})},\\qquad (1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where ${\\nabla f(x_{n})}$ denotes a gradient of $f$ evaluated at the iterate $x_{n}$, and $n$ is our iteration counter. As our step size rule, we pick a sequence that is square-summable but not summable, e.g., $\\gamma_{n}=1/n$, will do the job. \n",
    " \n",
    "We will go through the following steps:\n",
    "1. Load the packages\n",
    "2. Create the types\n",
    "3. Write the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the packages\n",
    "Let us load the necessary packages that we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the packages to be used\n",
    "# -----------------------------\n",
    "using ProximalOperators, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the types\n",
    "\n",
    "Next, we define a few Julia types, that we require to write an optimization solver in `Julia`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. ``GD_problem``\n",
    "\n",
    "This type contains information about the problem instance, this bascially tells us what function $f$ we are trying to optimize over, one initial point $x_0$, and what should be the beginning step size $\\gamma_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GD_problem \n",
    "    \n",
    "    # problem structure, contains information regarding the problem\n",
    "    \n",
    "    f # the objective function\n",
    "    x0 # the intial condition\n",
    "    γ # the stepsize\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage of ``GD_problem``.**  For example, the user may wish to solve a simple least-squares problem using gradient descent. Then he can create a problem instance. A list of functions that we can use in this regard can be found in the documentation of ``ProximalOperators``: [https://kul-forbes.github.io/ProximalOperators.jl/latest](https://kul-forbes.github.io/ProximalOperators.jl/latest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD_problem(description : Least squares penalty\n",
       "domain      : n/a\n",
       "expression  : n/a\n",
       "parameters  : n/a, [1.0193204973421695, 1.168794122203917, 1.296856638205154, 0.5001687528217552, 0.1500452441023732], 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a problem instance\n",
    "# ------------------------\n",
    "\n",
    "A = randn(6,5)\n",
    "\n",
    "b = randn(6)\n",
    "\n",
    "m, n = size(A)\n",
    "\n",
    "# randomized intial point:\n",
    "\n",
    "x0 = randn(n)\n",
    "\n",
    "f = LeastSquares(A, b)\n",
    "\n",
    "γ = 1.0\n",
    "\n",
    "# create GD_problem\n",
    "\n",
    "problem = GD_problem(f, x0, γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. ``GD_setting`` \n",
    "\n",
    "This type contains different parameters required to implement our algorithm, such as, \n",
    "\n",
    "* the initial step size $\\gamma$, \n",
    "* maximum number of iterations $\\textrm{maxit}$, \n",
    "* what should be the tolerance $\\textrm{tol}$ (i.e., if $\\| \\nabla{f(x)} \\| \\leq \\textrm{tol}$, we take that $x$ to be an optimal solution and terminate our algorithm), \n",
    "* whether to print out information about  the iterates or not controlled by a boolean variable $\\textrm{verbose}$, and \n",
    "* how frequently to print out such information controlled by the variable $\\textrm{freq}$.\n",
    "\n",
    "The user may specify what values for these parameters above should be used. But if he does not specify anything, we should be able to have a default set of values to be used. We can achieve this by creating a simple constructor function for ``GD_setting``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GD_setting\n",
    "    \n",
    "    # user settings to solve the problem using Gradient Descent\n",
    "    \n",
    "    γ # the step size\n",
    "    maxit # maximum number of iteration\n",
    "    tol # tolerance, i.e., if ||∇f(x)|| ≤ tol, we take x to be an optimal solution\n",
    "    verbose # whether to print information about the iterates\n",
    "    freq # how often print information about the iterates\n",
    "\n",
    "    # constructor for the structure, so if user does not specify any particular values, \n",
    "    # then we create a GD_setting object with default values\n",
    "    function GD_setting(; γ = 1, maxit = 1000, tol = 1e-8, verbose = false, freq = 10)\n",
    "        new(γ, maxit, tol, verbose, freq)\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage of ``GD_setting``.** For the previously described least squares problem, we create the following setting instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD_setting(1, 1000, 0.01, true, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setting = GD_setting(verbose = true, tol = 1e-2, maxit = 1000, freq = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. ``GD_state``\n",
    "Now we define the type named ``GD_state`` that describes the state our algorithm at iteration number $n$. The state is controlled by \n",
    "\n",
    "* current iterte $x_n$,\n",
    "* the gradient of $f$ at the current iterate: ${\\nabla{f}(x_n)}$,\n",
    "* the stepsize at iteration $n$: $\\gamma_n$, and\n",
    "* iteration number: $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct GD_state # contains information regarding one iterattion sequence\n",
    "    \n",
    "    x::Any # iterate x_n\n",
    "    ∇f_x::Any # one gradient ∇f(x_n)\n",
    "    γ::Any # stepsize\n",
    "    n::Any # iteration counter\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, once the user has given the problem information by creating a problem instance ``GD_problem``, we need a method to construct the intial value of the type `GD_state`,  as we did earlier for the least-squares problem. We create the intial state from the problem instance by writing a constructor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD_state"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GD_state(problem::GD_problem) \n",
    "    # a constructor for the struct GD_state, it will take the problem data and create one state containing all \n",
    "    # the iterate information, current state of the gradient etc so that we can start our gradient descent scheme\n",
    "    \n",
    "    # unpack information from iter which is GD_iterable type\n",
    "    x0 = problem.x0\n",
    "    f = problem.f\n",
    "    γ = problem.γ\n",
    "    ∇f_x, f_x = gradient(f, x0)\n",
    "    n = 1\n",
    "    \n",
    "    return GD_state(x0, ∇f_x, γ, n)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the functions \n",
    "\n",
    "Now that we are done defining the types, we can now focus on writing the functions that will implement our gradient descent scheme. \n",
    "\n",
    "#### 3.1. ```GD_iteration!```\n",
    "First, we need a function that will take the problem information and the state of our algorithm at iteration number $n$, and then compute the next state for iteration number $n+1$ according to (1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD_iteration! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GD_iteration!(problem::GD_problem, state::GD_state)\n",
    "    \n",
    "    # this is the main iteration function, that takes the problem information, and the previous state, \n",
    "    # and create the new state using Gradient Descent algorithm\n",
    "    \n",
    "    # unpack the current state information\n",
    "    x_n = state.x\n",
    "    ∇f_x_n = state.∇f_x\n",
    "    γ_n = state.γ\n",
    "    n = state.n\n",
    "    \n",
    "    # compute the next state\n",
    "    x_n_plus_1 = x_n - γ_n*∇f_x_n\n",
    "    \n",
    "    # now load the computed values in the state\n",
    "    state.x = x_n_plus_1\n",
    "    state.∇f_x, f_x = gradient(problem.f, x_n_plus_1) # note that f_x is not used anywhere\n",
    "    state.γ = 1/(n+1)\n",
    "    state.n = n+1\n",
    "    \n",
    "    # done computing return the new state\n",
    "    return state\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``GD_solver``\n",
    "Now we are in a position to write the main solver function named ``GD_solver`` that will be used by the end user. Internally, this function will take the problem information and the problem setting, and then it will\n",
    "\n",
    "* create the initial state,\n",
    "* keep updating the state using ``GD_iteration!`` function until we reach the termination criterion or the maximum number of iterations,\n",
    "* print state of the algorithm if ``verbose`` is ``true`` at the specified frequency, and \n",
    "* return the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD_solver (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The solver function\n",
    "\n",
    "function GD_solver(problem::GD_problem, setting::GD_setting)\n",
    "    \n",
    "    # this is the function that the end user will use to solve a particular problem, internally it is using the previously defined types and functions to run Gradient Descent Scheme\n",
    "    # create the intial state\n",
    "    state = GD_state(problem::GD_problem)\n",
    "    \n",
    "    ## time to run the loop\n",
    "    while  (state.n < setting.maxit) & (norm(state.∇f_x, Inf) > setting.tol)\n",
    "        # compute a new state\n",
    "        state =  GD_iteration!(problem, state)\n",
    "        # print information if verbose = true\n",
    "        if setting.verbose == true\n",
    "            if mod(state.n, setting.freq) == 0\n",
    "                @info \"iteration = $(state.n) |  \n",
    "                obj val = $(problem.f(state.x)) | \n",
    "                gradient norm = $(norm(state.∇f_x, Inf))\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # print information regarding the final state\n",
    "    \n",
    "    @info \"final iteration = $(state.n) | \n",
    "    final obj val = $(problem.f(state.x)) | \n",
    "    final gradient norm = $(norm(state.∇f_x, Inf))\"\n",
    "    return state\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage of ``GD_solver``.** For the previously created ``problem`` and ``setting``, we run our ``GD_solver`` function as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will run the entire loop over the struct GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: iteration = 100 |  \n",
      "│                 obj val = 0.10616975436389622 | \n",
      "│                 gradient norm = 0.02964822736954073\n",
      "└ @ Main In[9]:16\n",
      "┌ Info: iteration = 200 |  \n",
      "│                 obj val = 0.10526904472309405 | \n",
      "│                 gradient norm = 0.019909231214547213\n",
      "└ @ Main In[9]:16\n",
      "┌ Info: iteration = 300 |  \n",
      "│                 obj val = 0.10499388213419013 | \n",
      "│                 gradient norm = 0.01578344670011611\n",
      "└ @ Main In[9]:16\n",
      "┌ Info: iteration = 400 |  \n",
      "│                 obj val = 0.1048632843417343 | \n",
      "│                 gradient norm = 0.013388371065530952\n",
      "└ @ Main In[9]:16\n",
      "┌ Info: iteration = 500 |  \n",
      "│                 obj val = 0.10478781849295837 | \n",
      "│                 gradient norm = 0.011784780173541287\n",
      "└ @ Main In[9]:16\n",
      "┌ Info: iteration = 600 |  \n",
      "│                 obj val = 0.10473897267694926 | \n",
      "│                 gradient norm = 0.010618620536011203\n",
      "└ @ Main In[9]:16\n",
      "┌ Info: final iteration = 667 | \n",
      "│     final obj val = 0.10471495099365061 | \n",
      "│     final gradient norm = 0.009995364642054971\n",
      "└ @ Main In[9]:25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GD_state([-0.747429923239562, 0.7857409304268766, -1.9752613372498569, -0.8109189160513681, 0.6537705422543739], [-0.002114026011632561, -0.008995100759876307, 0.009995364642054971, 0.0024806200462658134, -0.004665182937515999], 0.0014992503748125937, 667)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state_GD = GD_solver(problem, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value found by our gradient descent 0.10471495099365061\n",
      "real objective value 0.10452813292628083 \n"
     ]
    }
   ],
   "source": [
    "println(\"objective value found by our gradient descent $(f(final_state_GD.x))\")\n",
    "\n",
    "println(\"real objective value $(f(pinv(A)*b)) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we do decent in terms of finding a good solution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
