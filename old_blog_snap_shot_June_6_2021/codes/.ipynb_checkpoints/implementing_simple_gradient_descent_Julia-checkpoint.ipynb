{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a simple gradient descent algorithm in ``Julia``"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this blog, we discuss how to implement a simple gradient descent scheme in ``Julia``. To do this, we will use the Julia package ``ProximalOperators``, which is an excellent package to compute proximal operators and gradient of common convex functions. I highly recommend the package for anyone interested in operator splitting algorithms. You can find more information about the package at: [https://github.com/kul-forbes/ProximalOperators.jl](https://github.com/kul-forbes/ProximalOperators.jl).\n",
        "\n",
        "Before we implement gradient descent method, we first record some necessary background.\n",
        "\n",
        "### 0. Background.\n",
        "\n",
        "Given a differentiable convex function $f$, our goal is to solve the following optimization problem: \n",
        "\n",
        "\\begin{eqnarray*}\n",
        "\\begin{array}{ll}\n",
        "\\textrm{minimize} & f(x)\\\\\n",
        "\\textrm{subject to} & x\\in\\mathbf{R}^{n},\n",
        "\\end{array}\n",
        "\\end{eqnarray*}\n",
        "\n",
        "where $x$ is the decision variable. To solve the problem above, we consider gradient descent algorithm. The gradient descent implements the following iteration scheme:\n",
        "\n",
        "\\begin{eqnarray} \\label{SGD}\n",
        "x_{n+1} & = & x_{n}-\\gamma_{n}{\\nabla f(x_{n})},\\qquad (1)\n",
        "\\end{eqnarray}\n",
        "\n",
        "where ${\\nabla f(x_{n})}$ denotes a gradient of $f$ evaluated at the iterate $x_{n}$, and $n$ is our iteration counter. As our step size rule, we pick a sequence that is square-summable but not summable, e.g., $\\gamma_{n}=1/n$, will do the job. \n",
        " \n",
        "We will go through the following steps:\n",
        "1. Load the packages\n",
        "2. Create the types\n",
        "3. Write the functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the packages\n",
        "Let us load the necessary packages that we are going to use."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the packages to be used\n",
        "# -----------------------------\n",
        "# comment the first two lines if you already have ProximalOperators\n",
        "using Pkg\n",
        "Pkg.add(\"ProximalOperators\")\n",
        "using ProximalOperators, LinearAlgebra\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `C:\\Users\\shuvo\\.julia\\registries\\General`\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
            "\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...===============>]  100.0 %.0 %\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m XML2_jll ─────── v2.9.9+4\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m IntervalSets ─── v0.5.0\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ImageMetadata ── v0.9.1\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ArrayInterface ─ v2.8.5\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ColorTypes ───── v0.10.0\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m AxisArrays ───── v0.4.0\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ImageCore ────── v0.8.14\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Colors ───────── v0.12.0\n",
            "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m FlameGraphs ──── v0.2.1\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `C:\\Users\\shuvo\\.julia\\environments\\v1.3\\Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `C:\\Users\\shuvo\\.julia\\environments\\v1.3\\Manifest.toml`\n",
            " \u001b[90m [4fba245c]\u001b[39m\u001b[93m ↑ ArrayInterface v2.8.4 ⇒ v2.8.5\u001b[39m\n",
            " \u001b[90m [39de3d68]\u001b[39m\u001b[95m ↓ AxisArrays v0.4.2 ⇒ v0.4.0\u001b[39m\n",
            " \u001b[90m [3da002f7]\u001b[39m\u001b[93m ↑ ColorTypes v0.9.1 ⇒ v0.10.0\u001b[39m\n",
            " \u001b[90m [5ae59095]\u001b[39m\u001b[93m ↑ Colors v0.11.2 ⇒ v0.12.0\u001b[39m\n",
            " \u001b[90m [08572546]\u001b[39m\u001b[93m ↑ FlameGraphs v0.2.0 ⇒ v0.2.1\u001b[39m\n",
            " \u001b[90m [4d00f742]\u001b[39m\u001b[95m ↓ GeometryTypes v0.7.10 ⇒ v0.7.6\u001b[39m\n",
            " \u001b[90m [a09fc81d]\u001b[39m\u001b[93m ↑ ImageCore v0.8.13 ⇒ v0.8.14\u001b[39m\n",
            " \u001b[90m [bc367c6b]\u001b[39m\u001b[93m ↑ ImageMetadata v0.9.0 ⇒ v0.9.1\u001b[39m\n",
            " \u001b[90m [8197267c]\u001b[39m\u001b[93m ↑ IntervalSets v0.4.0 ⇒ v0.5.0\u001b[39m\n",
            " \u001b[90m [02c8fc9c]\u001b[39m\u001b[93m ↑ XML2_jll v2.9.9+3 ⇒ v2.9.9+4\u001b[39m\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:06:56.747Z",
          "iopub.execute_input": "2020-04-15T16:06:57.963Z",
          "iopub.status.idle": "2020-04-15T16:08:18.223Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create the types\n",
        "\n",
        "Next, we define a few Julia types, that we require to write an optimization solver in `Julia`. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1. ``GD_problem``\n",
        "\n",
        "This type contains information about the problem instance, this bascially tells us what function $f$ we are trying to optimize over, one initial point $x_0$, and what should be the beginning step size $\\gamma_0$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "struct GD_problem{F <: ProximableFunction,A <: AbstractVecOrMat{<:Real}, R <: Real}\n",
        "    \n",
        "    # problem structure, contains information regarding the problem\n",
        "    \n",
        "    f::F # the objective function\n",
        "    x0::A # the intial condition\n",
        "    γ::R # the stepsize\n",
        "    \n",
        "end"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:08:33.366Z",
          "iopub.execute_input": "2020-04-15T16:08:33.373Z",
          "iopub.status.idle": "2020-04-15T16:08:33.485Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage of ``GD_problem``.**  For example, the user may wish to solve a simple least-squares problem using gradient descent. Then he can create a problem instance. A list of functions that we can use in this regard can be found in the documentation of ``ProximalOperators``: [https://kul-forbes.github.io/ProximalOperators.jl/latest](https://kul-forbes.github.io/ProximalOperators.jl/latest)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create a problem instance\n",
        "# ------------------------\n",
        "\n",
        "A = [0.0992104966854672 -0.6824105286448215 2.460409825989712 -0.004053803803441462 0.5715605356881739; \n",
        "    0.1742749398573746 2.5174383202611947 0.651567460427589 -0.5408271816480797 -0.01223347123544491; \n",
        "    1.0442129438578551 0.7352767582403392 -0.241436834090505 0.4638234650202825 0.23166711887400535; \n",
        "    -0.16776414819267987 0.08274565791466311 -0.6142796136746458 0.9830335769912758 0.3796261892272989; \n",
        "    0.3990439508203404 -0.47277912665344407 1.126410516294192 0.5625008522977397 -0.6990783609684885; \n",
        "    -1.7266177238831322 0.9984702196895198 -0.8542927758529749 1.5919321188500941 1.3677645133446954]\n",
        "\n",
        "# or generate a random A by running the following\n",
        "# A = randn(6,5)\n",
        "\n",
        "b = [-0.3965327124603881, -1.4820396287913964, -0.467961891048195, -1.555879871070049, 0.5014062010336909, 0.25719126443155527]\n",
        "\n",
        "# or, generate a random b by running \n",
        "# b = randn(6)\n",
        "\n",
        "m, n = size(A)\n",
        "\n",
        "# randomized intial point:\n",
        "\n",
        "x0 = randn(n)\n",
        "\n",
        "f = LeastSquares(A, b)\n",
        "\n",
        "γ = 1.0\n",
        "\n",
        "# create GD_problem\n",
        "\n",
        "problem = GD_problem(f, x0, γ)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": [
              "GD_problem{ProximalOperators.LeastSquaresDirect{Float64,Float64,Array{Float64,2},Array{Float64,1},Cholesky{Float64,Array{Float64,2}}},Array{Float64,1},Float64}(description : Least squares penalty\n",
              "domain      : n/a\n",
              "expression  : n/a\n",
              "parameters  : n/a, [1.1297778521319135, -1.5412078943916476, 0.22898702828884018, 0.30103847585081944, -1.0572035198699317], 1.0)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:15:01.083Z",
          "iopub.execute_input": "2020-04-15T16:15:01.093Z",
          "iopub.status.idle": "2020-04-15T16:15:01.121Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. ``GD_setting`` \n",
        "\n",
        "This type contains different parameters required to implement our algorithm, such as, \n",
        "\n",
        "* the initial step size $\\gamma$, \n",
        "* maximum number of iterations $\\textrm{maxit}$, \n",
        "* what should be the tolerance $\\textrm{tol}$ (i.e., if $\\| \\nabla{f(x)} \\| \\leq \\textrm{tol}$, we take that $x$ to be an optimal solution and terminate our algorithm), \n",
        "* whether to print out information about  the iterates or not controlled by a boolean variable $\\textrm{verbose}$, and \n",
        "* how frequently to print out such information controlled by the variable $\\textrm{freq}$.\n",
        "\n",
        "The user may specify what values for these parameters above should be used. But if he does not specify anything, we should be able to have a default set of values to be used. We can achieve this by creating a simple constructor function for ``GD_setting``."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "struct GD_setting\n",
        "    \n",
        "    # user settings to solve the problem using Gradient Descent\n",
        "    \n",
        "    γ::Float64 # the step size\n",
        "    maxit::Int64 # maximum number of iteration\n",
        "    tol::Float64 # tolerance, i.e., if ||∇f(x)|| ≤ tol, we take x to be an optimal solution\n",
        "    verbose::Bool # whether to print information about the iterates\n",
        "    freq::Int64 # how often print information about the iterates\n",
        "\n",
        "    # constructor for the structure, so if user does not specify any particular values, \n",
        "    # then we create a GD_setting object with default values\n",
        "    function GD_setting(; γ = 1, maxit = 1000, tol = 1e-8, verbose = false, freq = 10)\n",
        "        new(γ, maxit, tol, verbose, freq)\n",
        "    end\n",
        "    \n",
        "end"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:09:42.304Z",
          "iopub.execute_input": "2020-04-15T16:09:42.311Z",
          "iopub.status.idle": "2020-04-15T16:09:42.326Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage of ``GD_setting``.** For the previously described least squares problem, we create the following setting instance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "setting = GD_setting(verbose = true, tol = 1e-2, maxit = 1000, freq = 100)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": [
              "GD_setting(1.0, 1000, 0.01, true, 100)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:09:47.736Z",
          "iopub.execute_input": "2020-04-15T16:09:47.744Z",
          "iopub.status.idle": "2020-04-15T16:09:47.978Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. ``GD_state``\n",
        "Now we define the type named ``GD_state`` that describes the state our algorithm at iteration number $n$. The state is controlled by \n",
        "\n",
        "* current iterte $x_n$,\n",
        "* the gradient of $f$ at the current iterate: ${\\nabla{f}(x_n)}$,\n",
        "* the stepsize at iteration $n$: $\\gamma_n$, and\n",
        "* iteration number: $n$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mutable struct GD_state{T <: AbstractVecOrMat{<: Real}, I <: Integer, R <: Real} # contains information regarding one iterattion sequence\n",
        "    \n",
        "    x::T # iterate x_n\n",
        "    ∇f_x::T # one gradient ∇f(x_n)\n",
        "    γ::R # stepsize\n",
        "    n::I # iteration counter\n",
        "    \n",
        "end"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:10:03.935Z",
          "iopub.execute_input": "2020-04-15T16:10:03.941Z",
          "iopub.status.idle": "2020-04-15T16:10:03.954Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, once the user has given the problem information by creating a problem instance ``GD_problem``, we need a method to construct the intial value of the type `GD_state`,  as we did earlier for the least-squares problem. We create the intial state from the problem instance by writing a constructor function."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "function GD_state(problem::GD_problem)\n",
        "    \n",
        "    # a constructor for the struct GD_state, it will take the problem data and create one state containing all \n",
        "    # the iterate information, current state of the gradient etc so that we can start our gradient descent scheme\n",
        "    \n",
        "    # unpack information from iter which is GD_iterable type\n",
        "    x0 = copy(problem.x0) # to be safe\n",
        "    f = problem.f\n",
        "    γ = problem.γ\n",
        "    ∇f_x, f_x = gradient(f, x0)\n",
        "    n = 1\n",
        "    \n",
        "    return GD_state(x0, ∇f_x, γ, n)\n",
        "    \n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": [
              "GD_state"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:10:18.436Z",
          "iopub.execute_input": "2020-04-15T16:10:18.446Z",
          "iopub.status.idle": "2020-04-15T16:10:18.816Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Write the functions \n",
        "\n",
        "Now that we are done defining the types, we can now focus on writing the functions that will implement our gradient descent scheme. \n",
        "\n",
        "#### 3.1. ```GD_iteration!```\n",
        "First, we need a function that will take the problem information and the state of our algorithm at iteration number $n$, and then compute the next state for iteration number $n+1$ according to (1). "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "function GD_iteration!(problem::GD_problem, state::GD_state)\n",
        "    \n",
        "    # this is the main iteration function, that takes the problem information, and the previous state, \n",
        "    # and create the new state using Gradient Descent algorithm\n",
        "    \n",
        "    # unpack the current state information\n",
        "    x_n = state.x\n",
        "    ∇f_x_n = state.∇f_x\n",
        "    γ_n = state.γ\n",
        "    n = state.n\n",
        "    \n",
        "    # compute the next state\n",
        "    x_n_plus_1 = x_n - γ_n*∇f_x_n\n",
        "    \n",
        "    # now load the computed values in the state\n",
        "    state.x = x_n_plus_1\n",
        "    state.∇f_x, f_x = gradient(problem.f, x_n_plus_1) # note that f_x is not used anywhere\n",
        "\t# gradient(f,x) is a function in the ProximalOperators package, see its documentation \n",
        "\t# if more information is required\n",
        "    state.γ = 1/(n+1)\n",
        "    state.n = n+1\n",
        "    \n",
        "    # done computing return the new state\n",
        "    return state\n",
        "    \n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": [
              "GD_iteration! (generic function with 1 method)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:10:48.270Z",
          "iopub.execute_input": "2020-04-15T16:10:48.281Z",
          "iopub.status.idle": "2020-04-15T16:10:48.500Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ``GD_solver``\n",
        "Now we are in a position to write the main solver function named ``GD_solver`` that will be used by the end user. Internally, this function will take the problem information and the problem setting, and then it will\n",
        "\n",
        "* create the initial state,\n",
        "* keep updating the state using ``GD_iteration!`` function until we reach the termination criterion or the maximum number of iterations,\n",
        "* print state of the algorithm if ``verbose`` is ``true`` at the specified frequency, and \n",
        "* return the final state."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## The solver function\n",
        "\n",
        "function GD_solver(problem::GD_problem, setting::GD_setting)\n",
        "    \n",
        "    # this is the function that the end user will use to solve a particular problem, internally it is using the previously defined types and functions to run Gradient Descent Scheme\n",
        "    # create the intial state\n",
        "    state = GD_state(problem::GD_problem)\n",
        "    \n",
        "    ## time to run the loop\n",
        "    while  (state.n < setting.maxit) & (norm(state.∇f_x, Inf) > setting.tol)\n",
        "        # compute a new state\n",
        "        state =  GD_iteration!(problem, state)\n",
        "        # print information if verbose = true\n",
        "        if setting.verbose == true\n",
        "            if mod(state.n, setting.freq) == 0\n",
        "                @info \"iteration = $(state.n) | obj val = $(problem.f(state.x)) | gradient norm = $(norm(state.∇f_x, Inf))\"\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    \n",
        "    # print information regarding the final state\n",
        "    \n",
        "    @info \"final iteration = $(state.n) | final obj val = $(problem.f(state.x)) | final gradient norm = $(norm(state.∇f_x, Inf))\"\n",
        "    return state\n",
        "    \n",
        "end"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": [
              "GD_solver (generic function with 1 method)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:11:16.672Z",
          "iopub.execute_input": "2020-04-15T16:11:16.680Z",
          "iopub.status.idle": "2020-04-15T16:11:17.201Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage of ``GD_solver``.** For the previously created ``problem`` and ``setting``, we run our ``GD_solver`` function as follows.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The following function will run the entire loop over the struct GradientDescent"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "final_state_GD = GD_solver(problem, setting)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: final iteration = 15 | final obj val = 1.3426102641150683 | final gradient norm = 0.008021018664603696\n",
            "└ @ Main In[11]:23\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": [
              "GD_state{Array{Float64,1},Int64,Float64}([-0.5482448838438723, -0.42909865018108256, -0.0701068264712182, -0.07118625446430132, -0.5163734825032458], [0.0076930667784437246, -0.007012653935494384, 0.008021018664603696, -0.00661057576689994, 0.0011087604577533772], 0.06666666666666667, 15)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:15:13.827Z",
          "iopub.execute_input": "2020-04-15T16:15:13.835Z",
          "iopub.status.idle": "2020-04-15T16:15:13.852Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "println(\"objective value found by our gradient descent $(f(final_state_GD.x))\")\n",
        "\n",
        "println(\"real objective value $(f(pinv(A)*b)) \")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective value found by our gradient descent 1.3426102641150683\n",
            "real objective value 1.3425784868644 \n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-04-15T16:16:03.204Z",
          "iopub.execute_input": "2020-04-15T16:16:03.212Z",
          "iopub.status.idle": "2020-04-15T16:16:03.226Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we do decent in terms of finding a good solution!"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.3.0",
      "language": "julia",
      "name": "julia-1.3"
    },
    "language_info": {
      "file_extension": ".jl",
      "name": "julia",
      "mimetype": "application/julia",
      "version": "1.3.0"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}